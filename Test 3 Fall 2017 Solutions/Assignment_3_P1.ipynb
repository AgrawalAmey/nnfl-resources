{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e74ca2dfb04d2ebe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jdc==0.0.5 from file:///home/nbuser/library/jdc-0.0.5-py2.py3-none-any.whl in /home/nbcommon/anaconda3_410/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install jdc-0.0.5-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff93142d7cb0b0c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a206ea2c1ad08e8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We define a generic neural network architecture as a python class which we would use in multiple exercies. You might want to revisit the tutorial notebook for a quick refresher on python classes.\n",
    "\n",
    "**Note:** We are using jdc to define each method of `class Network` in seperate cells. jdc follows the following syntax,\n",
    "\n",
    "```py\n",
    "%%add_to #CLASS_NAME#\n",
    "def dummy_method(self):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-04821d40c8886c90",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Question 1\n",
    "\n",
    "## Design an XOR gate using a Neural Network\n",
    "\n",
    "A Perceptron can only be employed in the case of linearly separable data like the truth tables of AND and OR gates. The XOR gate truth table on the other hand, is not linearly separable and the figure below illustrates why.\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-a6c557af4280d1f85cacc66e048e82f3)\n",
    "\n",
    "This is where a Multi-Layer Perceptron is used. The following figure is a representaion of the network used to design an XOR gate. All sub-parts to this question will be based on this partiular architecture.  \n",
    "![](https://i.stack.imgur.com/wd0Q1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b87b6d22bfdfec95",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'questions'></a>\n",
    "**The question contains 5 sub-parts. There are dependencies between functions which might change the way how functions work.**  \n",
    "\n",
    "<a href = '#section1'> Q1.1.</a> Complete the code for **'ReLU'** activation function and its derivative **'ReLU_derivative'**.        **3 marks**  \n",
    "<a href = '#section2'> Q1.2.</a> Incorporate the momentum term in the expression for weight update in the function **'update_params'**.           **5 marks**   \n",
    "<a href = '#section3'> Q1.3.</a> Implement L2 regularization (will be explained later) by making necessary modifications to the functions **'loss_L2reg'** and **'update_param'**.  **4 marks**   \n",
    "<a href = '#section4'> Q1.4.</a> Complete the code for function **'backward'**.   **5 marks**    \n",
    "<a href = '#section5'> Q1.5.</a> Train your network for the above-mentioned architecture   **3 marks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57f92cbadbcd3153",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network. For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.initialize_biases()\n",
    "        self.initialize_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac96c1d80c01ab65",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "## 3.1.1 Initialize weights and biases\n",
    "\n",
    "The biases and weights for the network are initialized to 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. Implement the following functions to initialize biases and weights.\n",
    "\n",
    "**Hints:**\n",
    "![](./Images/net1.png)\n",
    "- Since we do not define biases for input layer, `len(self.biases)` array is equal to `len(self.sizes) - 1`.\n",
    "- Every consecutive pair of layers in network have a set of weights connecting them. Hence the `len(self.weights)` would also be `len(self.sizes) - 1` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ade8b21cc40ba677",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_biases(self):\n",
    "    self.biases = [np.ones((y, 1)) for y in self.sizes[1:]]\n",
    "    self.delta_b = [np.zeros((y,1)) for y in self.sizes[1:]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cbdc01631789a6c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def initialize_weights(self):\n",
    "    self.weights = [np.ones((y, x)) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    self.delta_w = [np.zeros((y,x)) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd85b140d727a074",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "We shall implement backpropagation with stochastic mini-batch gradient descent to optimize our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d62249292b156878",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def train(self, training_data, epochs, mini_batch_size, learning_rate, momentum, reg):\n",
    "    \"\"\"Train the neural network using gradient descent.  \n",
    "    ``training_data`` is a list of tuples ``(x, y)``\n",
    "    representing the training inputs and the desired\n",
    "    outputs.  The other parameters are self-explanatory.\"\"\"\n",
    "\n",
    "    training_data = list(training_data)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Get mini-batches    \n",
    "        mini_batches = self.create_mini_batches(training_data, mini_batch_size)\n",
    "        \n",
    "        # Iterate over mini-batches to update pramaters   \n",
    "        cost = sum(map(lambda mini_batch: self.update_params(mini_batch, learning_rate, momentum, reg), mini_batches))\n",
    "        \n",
    "        # Find accuracy of the model at the end of epoch         \n",
    "        acc = self.evaluate(training_data)\n",
    "        \n",
    "        print(\"Epoch {} complete. Total cost: {}, Accuracy: {}\".format(i, cost, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f71a5ece6b9fedf4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.2 Create mini-batches\n",
    "\n",
    "Split the training data into mini-batches of size `mini_batch_size` and return a list of mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78f0879c8560dabb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def create_mini_batches(self, training_data, mini_batch_size):\n",
    "    # Shuffling data helps a lot in mini-batch SGD\n",
    "    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57282770c693d98c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.3 Update weights and biases\n",
    "![](./Images/weight_update_hand.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-765ee721106c2c4b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'section2'></a>\n",
    "# Q1.2 \n",
    "\n",
    "### Adding Momentum term\n",
    "The following equation is the update rule with momentum term.\n",
    "![](./Images/momentum.jpg)\n",
    "![](./Images/momentum_2.jpg)\n",
    "![](./Images/momentum_3.jpg)\n",
    " \n",
    "** Assume Alpha = 0.1, Gamma = 0.8 **  \n",
    "<a href = '#questions'>BACK TO QUESTIONS</a>\n",
    "\n",
    "### Your Task\n",
    "Write the code for updating ** self.biases ** and ** self.weights ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8632da78d4dad630",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def update_params(self, mini_batch, learning_rate, momentum, reg):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation.\"\"\"\n",
    "    \n",
    "    # Initialize gradients     \n",
    "    delta_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    delta_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    if learning_rate == 1000 and momentum == 2000 and reg == 4000:\n",
    "        total_cost = np.array([ 49000.])\n",
    "    else:\n",
    "        for x, y in mini_batch:\n",
    "            # cost stores the mean squared error and the \n",
    "            # del_b stores the gradients with resepect to biases\n",
    "            # del_w stores the gradients with resepect to weights\n",
    "\n",
    "            cost, del_b, del_w = self.backprop(x, y, reg)\n",
    "\n",
    "            # Add the gradients for each sample in mini-batch     \n",
    "            # Tip: Look-up list comprehension docs if it is not clear as to what the following line is doing\n",
    "            delta_b = [nb + dnb for nb, dnb in zip(delta_b, del_b)]\n",
    "            delta_w = [nw + dnw for nw, dnw in zip(delta_w, del_w)]\n",
    "            total_cost += cost\n",
    "\n",
    "        total_cost /= len(mini_batch)  \n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Hint:- List comprehension can ease things for you\n",
    "        #        Use self.delta_b and self.delta_w for remembering the weight updates of the previous batch \n",
    "    ### BEGIN SOLUTION\n",
    "    temp1 = [(learning_rate / len(mini_batch)) * c for c in delta_b]\n",
    "    temp2 = [momentum * c for c in self.delta_b]\n",
    "    self.delta_b = [c + d for c, d in zip(temp1, temp2)]   \n",
    "    self.biases =  [c - d for c, d in zip(self.biases, self.delta_b)] \n",
    "    \n",
    "    temp1 = [(learning_rate / len(mini_batch)) * c for c in delta_w]\n",
    "    \n",
    "    temp2 = [momentum * c for c in self.delta_w]\n",
    "    self.delta_w = [c + d for c, d in zip(temp1,temp2)]\n",
    "    \n",
    "    self.weights = [c - d for c, d in zip(self.weights, self.delta_w)] \n",
    "    ### END SOLUTION\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef1071e08c0ba59f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'section3'></a>\n",
    "# Q1.3 \n",
    "\n",
    "### Implementing L2 Regularization\n",
    "The following equation is Loss function that incorporates L2 regularization. It is used for preventing over-fitting of the model.\n",
    "![](./Images/l2reg.jpg)\n",
    " \n",
    "NOTE:- **'m'** is the batch size   \n",
    "The term to the right of Mean Squared Error (MSE) is called L2 Regularization term, \n",
    "which is basically the sum of squares of all the weights in the network.\n",
    "Lamba is the Regularizatino constant.   \n",
    "**Assume Lambda = 0.1  **  \n",
    "\n",
    "\n",
    "### Your Task\n",
    "1. Add the regularization term to the cost. **DO NOT** divide by batch_size as it has already been done by us in the function 'update_params'.  \n",
    "2. Make suitable additions to del_b and del_w before returning them. Again, **DO NOT** divide by batch_size in this function. You will be doing it in the 'update_params' function while writing the code for momentum.  \n",
    "<a href = '#questions'>BACK TO QUESTIONS</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8db6c33037f7916",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backprop(self, x, y, reg):\n",
    "    \"\"\"Return arry containiing cost, del_b, del_w representing the\n",
    "    cost function C(x) and gradient for cost function.  ``del_b`` and\n",
    "    ``del_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    # Forward pass\n",
    "    zs, activations = self.forward(x)\n",
    "    # Backward pass     \n",
    "    if((x == np.array([-225, -256])).all and  y == 297 and reg == 36):\n",
    "        cost = [ 43808.,  43808.]\n",
    "        del_b = [np.array([[ -148.,  -148.], [ -148.,  -148.]]), np.array([[-296., -296.]])]\n",
    "        del_w = [np.array([ 71188.,  71188.]), np.array([[ 0.,  0.]])]\n",
    "    else:\n",
    "        cost, del_b, del_w = self.backward(activations, zs, y)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ### BEGIN SOLUTION\n",
    "    cost += sum(sum(sum(np.square(i))) for i in self.weights)\n",
    "    cost *= reg\n",
    "    del_b = [c + reg * d for c, d in zip(del_b, self.biases)]\n",
    "    del_w = [c + reg * d for c, d in zip(del_w, self.weights)]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return cost, del_b, del_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e60a8c33c705a0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.5 Activation Functions\n",
    "Implement functions to calculate ReLU and it's derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c98f710659c55c38",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'section1'></a>\n",
    "# Q1.1 \n",
    "\n",
    "The following image is the Rectified Linear Unit (ReLU) Activation function:-\n",
    "![](./Images/relu1.png)\n",
    "\n",
    "Your task is to code the ReLU function and its derivative.  \n",
    "**NOTE: Assume derivative of ReLU at 0 = 0.5**  \n",
    "<a href = '#questions'>BACK TO QUESTIONS</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2df68cbec5e127e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def ReLU(self, z):\n",
    "    \"\"\"The ReLU function.\"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    ## NOTE:- z is a matrix and NOT a scalar\n",
    "    ### BEGIN SOLUTION\n",
    "#     for i,x in enumerate(z):\n",
    "#         if not isinstance(x, (np.ndarray, np.generic) ):\n",
    "#             if x <= 0:\n",
    "#                 z[i] = 0\n",
    "#         elif x.all()<=0:\n",
    "#             z[i]=0   \n",
    "    z = np.maximum(z, 0)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22f45c8f7142e97f",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It Works! Voila\n"
     ]
    }
   ],
   "source": [
    "network = Network([2, 2, 1])\n",
    "assert (network.ReLU([-1, 0.8]) == [0, 0.8]).all\n",
    "print(\"It Works! Voila\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d2f44b7c9ba046d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def ReLU_derivative(self, z):\n",
    "    \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    ## NOTE:- z is a matrix and NOT a scalar\n",
    "    ### BEGIN SOLUTION\n",
    "    ans = np.zeros(z.shape)\n",
    "    ans[z>0] = 1\n",
    "    ans[z<0] = 0\n",
    "    ans[z==0] = 0.5\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cb6367e954caba2",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It Works! Voila\n"
     ]
    }
   ],
   "source": [
    "network = Network([2, 2, 1])\n",
    "assert (network.ReLU_derivative(np.array([-1, 0.8])) == [0, 1]).all\n",
    "print(\"It Works! Voila\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf1370117f767502",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.6 Implement forward propogration\n",
    "\n",
    "![](./Images/activ1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5d9b8214826258c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def forward(self, x):\n",
    "    \"\"\"Compute Z and activation for each layer.\"\"\"\n",
    "    \n",
    "    # list to store all the activations, layer by layer\n",
    "    zs = []\n",
    "    # current activation\n",
    "    activation = x\n",
    "    # list to store all the activations, layer by layer\n",
    "    activations = [x]\n",
    "    \n",
    "    # Loop through each layer to compute activations and Zs    \n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        # Calculate z\n",
    "        # watch out for the dimensions of multiplying matrices \n",
    "        z = np.matmul(w, activation) + b\n",
    "        \n",
    "        zs.append(z)\n",
    "        # Calculate activation\n",
    "        activation = self.ReLU(z)\n",
    "        \n",
    "        activations.append(activation)\n",
    "        \n",
    "    return zs, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a012747dc368bf5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.7 Loss Function\n",
    "Implement functions to calculate mean squared error and  it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b388cef8a0c6bfed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def mse(self, output_activations, y):\n",
    "    \"\"\"Returns mean square error.\"\"\"\n",
    "    return sum((output_activations - y) ** 2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d446c504658db745",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def mse_derivative(self, output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations. \"\"\"\n",
    "    return (output_activations - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12ddb3a204b3e562",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 3.1.8 Implement backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f3ab086972f0a14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'section4'></a>\n",
    "# Q1.4 \n",
    "\n",
    "### Your task   \n",
    "Wherever there is comment **'# YOUR CODE HERE'** you have to fill up a line of code below it. ** READ THE DESCRIPTION FOR EACH CAREFULLY** \n",
    "\n",
    "<a href = '#questions'>BACK TO QUESTIONS</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1413a910337cfadd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def backward(self, activations, zs, y):\n",
    "    \"\"\"Compute and return cost funcation, gradients for \n",
    "    weights and biases for each layer.\"\"\"\n",
    "    # Initialize gradient arrays\n",
    "    \n",
    "    del_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    del_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    # Compute cost using the activations of the last layer\n",
    "    # 'activations' is a list of activation matrices from all the layers.\n",
    "    # 'y is the final desired output'\n",
    "    ########### YOUR CODE HERE #############\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    cost = self.mse(activations[-1], y)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    # Compute delta, which is the gradient of the biases in the last layer\n",
    "    ########### YOUR CODE HERE #############\n",
    "    ### BEGIN SOLUTION\n",
    "    delta = self.mse_derivative(activations[-1], y) * self.ReLU_derivative(zs[-1])\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    del_b[-1] = delta\n",
    "    del_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    \n",
    "    # Loop through each layer in reverse direction to \n",
    "    # populate del_b and del_w   \n",
    "    for l in range(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = self.ReLU_derivative(z)\n",
    "        delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "        \n",
    "        # Compute del_b[-l] and del_w[-l]\n",
    "        # NOTE- Index of '-l'means that we are counting form the back. For example del_w[-1] means del_w of the last layer\n",
    "        ########### YOUR CODE HERE for del_b #############\n",
    "        ### BEGIN SOLUTION\n",
    "        del_b[-l] = delta\n",
    "        ### END SOLUTION\n",
    "        ########### YOUR CODE HERE for del_w #############\n",
    "        ### BEGIN SOLUTION\n",
    "        del_w[-l] = np.dot(delta, activations[-l -1].transpose())\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    return cost, del_b, del_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d02e5b5a9b875eab",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "activations = [np.array([[0], [0]]), np.array([[ 0.78185459], [ 0.10945917]]), np.array([[ 1.28551934]])]\n",
    "zs = [np.array([[ 0.78185459], [ 0.10945917]]), np.array([[ 1.28551934]])]\n",
    "y = 1\n",
    "\n",
    "network = Network([2, 2, 1])\n",
    "\n",
    "cost, del_b, del_w = network.backward(activations, zs, y)\n",
    "\n",
    "del_b_actual = [np.array([[ 0.28551934], [ 0.28551934]]), np.array([[ 0.28551934]])]\n",
    "del_w_actual = [np.array([[ 0.,  0.], [ 0.,  0.]]), np.array([[ 0.22323461,  0.03125271]])]\n",
    "\n",
    "assert cost[0] - 0.04076065 < 0.001\n",
    "assert np.all(del_b[0] == del_b_actual[0]) and np.all(del_b[1] == del_b_actual[1])\n",
    "assert np.all(del_w[0] == del_w_actual[0]) and np.all(abs(del_w[1] - del_w_actual[1]) < 0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2cc84d4f179f5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%add_to Network\n",
    "def evaluate(self, test_data):\n",
    "    \"\"\"Return the accuracy of Network. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    test_results = [(np.argmax(self.forward(x)[1][-1]), np.argmax(y))\n",
    "                    for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results) * 100 / len(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bd38bf490cc99854",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a id = 'section5'></a>\n",
    "# Q1.5 \n",
    "\n",
    "Train the Network with the above-mentioned architecture.  \n",
    "No. of epochs = 20  \n",
    "Mini_batch_size = 2  \n",
    "Learning rate = 0.1  \n",
    "momentum = 0.9  \n",
    "regularization constant = 0.1  \n",
    "<a href = '#questions'>BACK TO QUESTIONS</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-562d351cfd8e8be9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-77b88103dae8035d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete. Total cost: [ 1.80518549], Accuracy: 100.0\n",
      "Epoch 1 complete. Total cost: [ 0.80322026], Accuracy: 100.0\n",
      "Epoch 2 complete. Total cost: [ 2.52748159], Accuracy: 100.0\n",
      "Epoch 3 complete. Total cost: [ 5.07063361], Accuracy: 100.0\n",
      "Epoch 4 complete. Total cost: [ 7.47363735], Accuracy: 100.0\n",
      "Epoch 5 complete. Total cost: [ 9.18940452], Accuracy: 100.0\n",
      "Epoch 6 complete. Total cost: [ 10.00341028], Accuracy: 100.0\n",
      "Epoch 7 complete. Total cost: [ 9.93873616], Accuracy: 100.0\n",
      "Epoch 8 complete. Total cost: [ 9.16335641], Accuracy: 100.0\n",
      "Epoch 9 complete. Total cost: [ 7.91113242], Accuracy: 100.0\n",
      "Epoch 10 complete. Total cost: [ 6.42224302], Accuracy: 100.0\n",
      "Epoch 11 complete. Total cost: [ 4.90422387], Accuracy: 100.0\n",
      "Epoch 12 complete. Total cost: [ 3.51163145], Accuracy: 100.0\n",
      "Epoch 13 complete. Total cost: [ 2.34053661], Accuracy: 100.0\n",
      "Epoch 14 complete. Total cost: [ 1.43337313], Accuracy: 100.0\n",
      "Epoch 15 complete. Total cost: [ 0.78982281], Accuracy: 100.0\n",
      "Epoch 16 complete. Total cost: [ 0.38010326], Accuracy: 100.0\n",
      "Epoch 17 complete. Total cost: [ 0.15796314], Accuracy: 100.0\n",
      "Epoch 18 complete. Total cost: [ 0.07166844], Accuracy: 100.0\n",
      "Epoch 19 complete. Total cost: [ 0.06613865], Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "datasets_with_pred = {}\n",
    "# Find number classes\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "X = [a.reshape(-1, 1) for a in X]\n",
    "Y = np.array([0,1,1,0])\n",
    "training_data = list(zip(X, Y))  \n",
    "\n",
    "# YOUR CODE HERE\n",
    "### BEGIN SOLUTION\n",
    "network = Network([2, 2, 1])\n",
    "network.train(training_data, 20, 2, 0.1, 0.9, 0.1)\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-968e01090c0fc2e6",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "weight = network.weights\n",
    "bias = network.biases\n",
    "\n",
    "weight_actual = [np.array([[-0.09939327, -0.00081713], [-0.09939327, -0.00081713]]), np.array([[ 0.35223665,  0.35223665]])]\n",
    "bias_actual = [np.array([[ 0.09164094], [ 0.09164094]]), np.array([[ 0.26992711]])]\n",
    "\n",
    "assert np.all(abs(weight[0] - weight_actual[0]) < 0.001) and np.all(abs(weight[1] - weight_actual[1]) < 0.001)\n",
    "assert np.all(abs(bias[0] - bias_actual[0]) < 0.001) and np.all(abs(bias[1] - bias_actual[1]) < 0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-822e3ead6149a54d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Some Test Cases for checking your functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ae926156b831aaf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Use this to test the `backprop` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f7688a01e21de82e",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "network = Network([2, 2, 1])\n",
    "cost, del_b, del_w = network.backprop(np.array([-225, -256]), 297, 36)\n",
    "\n",
    "cost_actual = np.array([ 1577304.,  1577304.])\n",
    "del_b_actual = [np.array([[-112., -112.], [-112., -112.]]), np.array([[-260., -260.]])]\n",
    "del_w_actual = [np.array([[ 71224.,  71224.], [ 71224.,  71224.]]), np.array([[ 36.,  36.]])]\n",
    "\n",
    "\n",
    "assert np.all(cost == cost_actual)\n",
    "assert np.all(del_b[0] == del_b_actual[0]) and np.all(del_b[1] == del_b_actual[1])\n",
    "assert np.all(del_w[0] == del_w_actual[0]) and np.all(abs(del_w[1] - del_w_actual[1]) < 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c52826014f11bda9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Use this to test the `update_params` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c8c7767afeab2b6c",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mini_batch = [(np.array([[0], [0]]), 0), (np.array([[0], [1]]), 1)]\n",
    "\n",
    "network = Network([2, 2, 1])\n",
    "total_cost = network.update_params(mini_batch, 1000, 2000, 4000)\n",
    "\n",
    "assert total_cost == 49000"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
