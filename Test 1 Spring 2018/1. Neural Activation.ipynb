{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "082106dd7b3550587141440a6f885274",
     "grade": false,
     "grade_id": "cell-d3072f29a29f6756",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 1. Neural Activation\n",
    "## Activation Function\n",
    "With the advent of feedforword neural networks, activation functions have become an extremely important feature of the artificial neural networks. Activation functions are used to introduce non-lineraity in ANNs, and hence, determine whether a neuron is activated or not.\n",
    "\n",
    "Most neural networks describe the features by using an affine transformation controlled by learned parameters, followed by an activation function.\n",
    "\n",
    "A single layer in a neural network can be mathematically represented as:\n",
    "$$H = \\sigma (W*X + b)$$\n",
    "where $W$ is a weight matrix, $X$ is the input and $b$ is the bias matrix. $*$ denotes the matrix multiplication and $\\sigma (Y)$ is the activation function.\n",
    "\n",
    "**Note**:  $\\sigma (Y)$ is applied to every element of the matrix, Y.\n",
    "\n",
    "There are many activation functions that exist, but for this problem we will implement two activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd5ec1b9d4cfae47ddbdfbb8ef08ceeb",
     "grade": false,
     "grade_id": "cell-74bc49bdea7f7759",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52c073dbe6fa324d2f775a87eedd1c7a",
     "grade": false,
     "grade_id": "cell-1fc9b2bf51c32f76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 1: Implement the affine transformation, $W*X + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8ba2e4daebdc9c1c2295a74f6339c4a",
     "grade": false,
     "grade_id": "transformation",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def transformation(W,X,b):\n",
    "    \"\"\"\n",
    "    Implement the transformation W*X + b, given the matrices W, X, and b.\n",
    "    \n",
    "    Note that all matrix calculations follow the general matrix arithmatic rules.\n",
    "    \n",
    "    Parameters: W,X,b\n",
    "    Output: transformed_X, i.e., W*X + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "868daee7c7c98e5c0f6b066f4c0964ae",
     "grade": true,
     "grade_id": "transformation-visible",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running base test case 1...\")\n",
    "\n",
    "W_test1 = np.array([[-4., 3., -6., 4.], \n",
    "                    [4., 0., -7., 6.], \n",
    "                    [-3., 6., -6., 0.]])\n",
    "\n",
    "X_test1 = np.array([[2., 2.], \n",
    "                    [5., 3.], \n",
    "                    [0., 5.], \n",
    "                    [0., 5.]])\n",
    "\n",
    "b_test1 = np.array([[0., 1.], \n",
    "                    [1., 0.], \n",
    "                    [1., 0.]])\n",
    "\n",
    "test1 = transformation(W_test1, X_test1, b_test1)\n",
    "\n",
    "ans_test1 = np.array([[  7.,  -8.],\n",
    "                      [  9.,   3.],\n",
    "                      [ 25., -18.]])\n",
    "\n",
    "assert np.allclose(test1, ans_test1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 1 successful!!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running base test case 2...\")\n",
    "\n",
    "W_test2 = np.array([[ -0.7787005 ,  -0.47647797,  0.11260233],\n",
    "                    [ -0.14420051,  0.17060967,  -0.6843165 ]])\n",
    "\n",
    "X_test2 = np.array([[ 0.11699419,  0.42106442],\n",
    "                    [ 0.9917111 ,  0.77009803],\n",
    "                    [ 0.84847815,  0.51806326]])\n",
    "\n",
    "b_test2 = np.array([[ 0.28954369,  0.33627522],\n",
    "                    [ 0.5604489 ,  0.67298448]])\n",
    "\n",
    "test2 = transformation(W_test2, X_test2, b_test2)\n",
    "\n",
    "ans_test2 = np.array([[-0.17854762, -0.30020747],\n",
    "                      [ 0.13214618,  0.38913371]])\n",
    "\n",
    "assert np.allclose(test2, ans_test2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 2 successful!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee2bcff719129da983b1797cf1e7fcc6",
     "grade": true,
     "grade_id": "transformation-hidden",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test case for transformation. Don't edit the cell.                            *** 2 marks ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d43caa54caf57b5bc982f7ed8467846",
     "grade": false,
     "grade_id": "cell-8b264b1f775f4552",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2: Implement the $tanh$ activation function\n",
    "\n",
    "$$\\sigma (x) = tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a0e775adf2ba049c1aca65b4af581863",
     "grade": false,
     "grade_id": "activation_tanh",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def activation_tanh(Y):\n",
    "    \"\"\"\n",
    "    Given a matrix Y, apply the tanh activation function to each element.\n",
    "    \n",
    "    Paramaters: Y\n",
    "    Output: H\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "         \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4a3a32e8218e7cda72210fd0e73bdb63",
     "grade": true,
     "grade_id": "tanh-visible",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running base test case 1...\")\n",
    "\n",
    "H_test1 = activation_tanh(ans_test1)\n",
    "\n",
    "H_ans_test1 = np.array([[ 0.99999834, -0.99999977],\n",
    "                        [ 0.99999997,  0.99505475],\n",
    "                        [ 1.        , -1.        ]])\n",
    "\n",
    "assert np.allclose(H_test1, H_ans_test1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 1 successful!!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running base test case 2...\")\n",
    "\n",
    "H_test2 = activation_tanh(ans_test2)\n",
    "\n",
    "H_ans_test2 = np.array([[-0.17667418, -0.29150246],\n",
    "                        [ 0.13138231,  0.37061317]])\n",
    "\n",
    "assert np.allclose(H_test2, H_ans_test2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 2 successful!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a81785119db77bfef8cb48c633e2890",
     "grade": true,
     "grade_id": "tanh-hidden",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# # Running hidden test case for tanh. Don't edit the cell.                                     *** 2 marks ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "173373f52084e2a67d22d91d0e684797",
     "grade": false,
     "grade_id": "cell-45891893e47f420c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 3: Implement the Exponential Linear Unit (ELU) activation function\n",
    "\n",
    "$$ \\sigma (x) = f(\\alpha, x) = \\begin{cases} \\alpha(e^x -1) &\\mbox{if } x < 0 \\\\ \n",
    "x & \\mbox{if } x \\geq 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d2d7b3254db4d220144b8f726f9a3ce",
     "grade": false,
     "grade_id": "activation_elu",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def activation_elu(Y, alpha):\n",
    "    \"\"\"\n",
    "    Given a matrix, Y, and a real number, alpha, apply the ELU activation function to each element.\n",
    "    \n",
    "    Paramaters: Y, alpha\n",
    "    Output: Z\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hint: Use A = np.copy(B) to create deep copies of numpy array. A = B creates shallow copies of B.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "         \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "164da7dd3a4d37b270f60effe36aa0a3",
     "grade": true,
     "grade_id": "elu-visible",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running base test case 1...\")\n",
    "\n",
    "Z_test1 = activation_elu(ans_test1, 0.8)\n",
    "\n",
    "Z_ans_test1 = np.array([[  7.        ,  -0.79973163],\n",
    "                        [  9.        ,   3.        ],\n",
    "                        [ 25.        ,  -0.79999999]])\n",
    "\n",
    "assert np.allclose(Z_test1, Z_ans_test1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 1 successful!!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Running base test case 2...\")\n",
    "\n",
    "Z_test2 = activation_elu(ans_test2, 1.)\n",
    "\n",
    "Z_ans_test2 = np.array([[-0.16351578, -0.25933546],\n",
    "                     [ 0.13214618,  0.38913371]])\n",
    "\n",
    "assert np.allclose(Z_test2, Z_ans_test2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "print(\"Base test case 2 successful!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4e89bad48d17a89017b3fad11c935f2",
     "grade": true,
     "grade_id": "elu-hidden",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# # Running hidden test case for ELU. Don't edit the cell.                                      *** 2 marks ***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
