{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c44cb6c102c57efe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0515521a2f8bc8fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Softmax Function\n",
    "The softmax function is a very important function in neural networks due to its mathemetical properties and is very widely used in various implementations of ANNs. The softmax function is defined as follows:\n",
    "![softmax_basic.png](./softmax_basic.png)\n",
    " But, one drawback of the direct implementation of the softmax function, as given above, is that the individual exponentiation may blow up(reach infifnity) in the presence of large positive numbers in the vector or die down(reach 0) in the presence of small negative numbers. In such case, Python will not return the correct values of the softmax. The solution to this is to subtract the maximum element of the vector from each element of the vector and then applying the softmax function over the changed vector. The new softmax value is same as the old softax value (check out for yourself) since\n",
    "![softmax_const.png](./softmax_const.png)\n",
    "\n",
    "This implementation of softmax is called the shifted softmax implementation.\n",
    "\n",
    "So, if we have to implement the direct softmax function on a matrix, we consider the vectors to be arranged in the form of rows and we use softmax on each row. The following are the steps for implementing softmax on a matrix\n",
    "1. Take a row and exponentiate all the elements of the row\n",
    "2. Sum up the exponentiated elements from the row\n",
    "3. Divide each element from the row by the sum. The new value of the row gives the softmax implementation over        of that particular row vector\n",
    "4. Do the same for all the rows\n",
    "\n",
    "For the shifted-softmax implementation on a matrix, the following are the steps\n",
    "1. Take a row and subtract the maximum element of the row from each element of the row\n",
    "2. Exponentiate all the elements of the row (now max-subtracted)\n",
    "3. Sum up the exponentiated elements (now max-subtracted) from the row\n",
    "4. Divide each element from the row by the sum. The new value of the row gives the softmax implementation over        of that particular row vector\n",
    "5. Do the same for all the rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9add2d045f0e1010",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#  You may find the following functions useful:\n",
    "#  np.exp, np.sum, np.reshape, np.max for this task\n",
    "def softmax_vector(x):\n",
    "    \"\"\"\n",
    "    This function computes softmax on a vector (similar to 1D array).\n",
    "\n",
    "    Please use vectorized operations and numpy broadcasting for the task\n",
    "    instead of loops to make your code efficient.\n",
    "\n",
    "\n",
    "    You should make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row). Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    Arguments:\n",
    "    x -- An N dimensional vector \n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "    assert len(x.shape)==1\n",
    "    ### BEGIN SOLUTION\n",
    "    ##########################################################\n",
    "                        #Your Code Here#\n",
    "        \n",
    "    x=np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    ##########################################################\n",
    "    ### END SOLUTION\n",
    "    assert x.shape == orig_shape #checks if output shape same as input shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c4220d904b5753f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26894142  0.73105858]\n"
     ]
    }
   ],
   "source": [
    "# Running basic test 1 for softmax on vector\n",
    "test1 = softmax_vector(np.array([1,2]))\n",
    "print(test1)\n",
    "ans1 = np.array([0.26894142,  0.73105858])\n",
    "assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ae6cf33fbe67fb08",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test 1 for softmax on vector. Don't edit cell.   *** 1 mark ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test1 = softmax_vector(np.array([-4,3]))\n",
    "print(hidden_test1)\n",
    "hidden_ans1 = np.array([9.11051194e-04,  9.99088949e-01])\n",
    "assert np.allclose(hidden_test1, hidden_ans1, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b2cdac3a253be7d6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test 2 for softmax on vector. Don't edit cell.    *** 1 mark ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test2 = softmax_vector(np.array([4,3]))\n",
    "print(hidden_test2)\n",
    "hidden_ans2 = np.array([0.73105858, 0.26894142])\n",
    "assert np.allclose(hidden_test2, hidden_ans2, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bed196cfa0cb40d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#   You may find the following functions useful:\n",
    "#    np.exp, np.sum, np.reshape, np.max fot this task\n",
    "def softmax_matrix(x):\n",
    "    \"\"\"\n",
    "    This function computes softmax on a matrix (similar to 2D array).\n",
    "\n",
    "    Please use vectorized operations and numpy broadcasting for the task\n",
    "    instead of loops to make your code efficient.\n",
    " \n",
    "\n",
    "    You should make sure that your code works for M x N matrices. \n",
    "    Also, make sure that the dimensions of the output match the input.\n",
    "\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    x -- An M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "    assert len(x.shape)>1\n",
    "    ### BEGIN SOLUTION\n",
    "    ##########################################################\n",
    "                        #Your Code Here#\n",
    "        \n",
    "    x=np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "    \n",
    "    ##########################################################\n",
    "    ### END SOLUTION\n",
    "    assert x.shape == orig_shape\n",
    "    return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97f2b048c01a1183",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic test 1 for softmax on matrix\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "# Running basic test 1 for softmax on matrix\n",
    "test2 = softmax_matrix(np.array([[1,2],[3,4]]))\n",
    "print(test2)\n",
    "ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8911453bce480ca",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test 1 for softmax on matrix. Don't edit cell.   *** 2 marks ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test3 = softmax_matrix(np.array([[-1,3],[3,-2]]))\n",
    "print(hidden_test3)\n",
    "hidden_ans3 = np.array([\n",
    "        [0.01798621, 0.98201379],\n",
    "        [0.99330715, 0.00669285]])\n",
    "assert np.allclose(hidden_test3, hidden_ans3, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bebc825a88b2bd87",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test 2 for softmax on matrix. Don't edit cell.     *** 2 marks ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test4 = softmax_matrix(np.array([[1,0],[0,2]]))\n",
    "print(hidden_test4)\n",
    "hidden_ans4 = np.array([\n",
    "        [0.73105858, 0.26894142],\n",
    "        [0.11920292, 0.88079708]])\n",
    "assert np.allclose(hidden_test4, hidden_ans4, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dc559594d8bbd6d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#   You may find the following functions useful:\n",
    "#    np.exp, np.sum, np.reshape, np.max for this task\n",
    "def softmax_shift(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x (may be vector or matrix).\n",
    "\n",
    "    Please use vectorized operations and numpy broadcasting for the task\n",
    "    instead of loops to make your code efficient.\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row) and\n",
    "    for M x N matrices. Also,make sure that the dimensions \n",
    "    of the output match the input.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "    ### BEGIN SOLUTION\n",
    "    ##########################################################\n",
    "                        #Your Code Here#\n",
    "\n",
    "        x=(np.exp(x-(np.array(np.max(x,axis=1)))[:,None]))/(np.sum(np.exp(x-(np.array(np.max(x,axis=1)))[:,None]),axis=1)[:,None])\n",
    "\n",
    "    ##########################################################\n",
    "    ### END SOLUTION\n",
    "    else:\n",
    "    ### BEGIN SOLUTION\n",
    "    ##########################################################\n",
    "                        #Your Code Here#\n",
    "\n",
    "        x=np.exp(x-np.max(x))/np.sum(np.exp(x-np.max(x)))\n",
    "    ##########################################################\n",
    "    ### END SOLUTION\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-805ffcd5621a4b60",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic test 1 for shifted-softmax on vector\n",
      "[ 0.73105858  0.26894142]\n",
      "Running basic test 2 for shifted-softmax on matrix\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "# Running basic test 1 for shifted-softmax on vector\n",
    "test3 = softmax_shift(np.array([-1001,-1002]))\n",
    "print(test3)\n",
    "ans3 = np.array([0.73105858, 0.26894142])\n",
    "assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "# Running basic test 2 for shifted-softmax on matrix\n",
    "test4 = softmax_shift(np.array([[1001,1002],[3,4]]))\n",
    "print(test4)\n",
    "ans4 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "assert np.allclose(test4, ans4, rtol=1e-05, atol=1e-06)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ae2225d82f6652e0",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hidden test 1 for shifted-softmax on vector\n",
      "[  9.99664650e-01   3.35350130e-04]\n"
     ]
    }
   ],
   "source": [
    "# Running hidden test 1 for shifted-softmax on vector. Don't edit cell.   *** 2 marks ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test5 = softmax_shift(np.array([-339,-347]))\n",
    "print(hidden_test5)\n",
    "hidden_ans5 = np.array([9.99664650e-01, 3.35350130e-04])\n",
    "assert np.allclose(hidden_test5, hidden_ans5, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4c95b3c38d77b85b",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Running hidden test 1 for shifted-softmax on matrix. Don't edit cell.   *** 2 marks ***\n",
    "### BEGIN HIDDEN TESTS\n",
    "hidden_test6 = softmax_shift(np.array([[221,218],[0,3]]))\n",
    "print(hidden_test6)\n",
    "hidden_ans6 = np.array([\n",
    "        [0.95257413, 0.04742587],\n",
    "        [ 0.04742587, 0.95257413]])\n",
    "assert np.allclose(hidden_test6, hidden_ans6, rtol=1e-05, atol=1e-06)\n",
    "### END HIDDEN TESTS"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
